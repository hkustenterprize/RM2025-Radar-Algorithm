# HKUST ENTERPRIZE RoboMaster 2025 Radar Algorithm
<p align="center">
<a href="./LICENSE"><img alt="License" src="https://img.shields.io/badge/License-MIT-yellow"></a>
</p>

<p align="center">
  <img src="./docs/logo.png" width="300"/>
</p>


## 开源协议
本项目基于[MIT许可证](LICENSE)开源。引用时请注明项目来源：香港科技大学ENTERPRIZE战队RoboMaster 2025雷达站算法开源项目。



## 项目概述

雷达站作为RoboMaster比赛中的关键辅助兵种，需要在复杂赛场环境中稳定追踪双方机器人的运动状态。本系统通过创新的算法设计，解决了传统方案在遮挡处理、非平面场地定位、身份持续追踪等方面的痛点，在2025赛季复活赛中表现优异，助力战队取得单局最高1912.1s易伤时间、局均1618.3s易伤时间和420.2额外伤害的成绩。

**核心功能**：
- 高精度机器人检测（支持遮挡、低光照等复杂场景）
- 三维空间精准定位（适配多层台阶、斜坡等非平面场地）
- 稳定的多目标追踪（解决快速移动、临时遮挡导致的身份混淆）
- 战术辅助数据输出（为决策系统提供实时轨迹与状态信息）


## 算法流程

![算法概览图](docs/teaser.png)
*图1：雷达站算法整体流程*

系统分为两大核心模块：
1. **检测模块（Detector）**：基于深度学习模型提取机器人及装甲板信息
2. **追踪模块（Tracker）**：通过匹配算法维持目标身份与轨迹连续性


## 核心技术亮点

### 1. 专用数据集构建

针对比赛场景特点，构建包含复杂工况的高质量数据集：
- 数据来源：官方直播图像、各战队开源第一视角视频
- 标注内容：机器人整体框、装甲板位置、颜色、存活状态（含死亡状态标注）
- 关键优化：引入天然遮挡样本（非人工mask增强），显著提升模型鲁棒性

![数据集示例](docs/official_notation.png)
*图2：含遮挡场景的数据集样本与标注效果。从官方的图片直播中获取了大量带机器人遮挡的原始数据并进行了人工标注，相比训练时引入遮挡等数据增强，天然的遮挡训练集能大大增加模型对赛场复杂情况下的适应性。*

### 2. 多层检测网络

采用三级检测架构，平衡精度与速度：
- **机器人检测**：YOLOv12-s模型（输入1280×1280），优化小目标检测能力
- **装甲板检测**：YOLOv12-n模型（输入192×192），输出位置与状态信息
- **图案分类**：MobileNetv2轻量网络（输入64×64），分类准确率达99.99%

### 3. 与传统方案对比

| 方案 | 优势 | 局限性 |
|------|------|--------|
| 传统透视变换 | 计算简单 | 依赖平面假设，非平面区域误差大 |
| 多层透视变换 | 支持简单分层场地 | 层间交界模糊，需大量标定 |
| 本方案射线投射 | 精准适配3D场地，无需分层 | 依赖精确场地模型 |

### 4. 射线投射定位法

创新的三维定位方案，解决传统透视变换在非平面场地的局限性：
- 原理：从相机光心发射射线，与预加载的场地3D网格模型求交，获取世界坐标
- 优势：
  - 无需分层处理，直接适配高地、台阶等复杂地形
  - 输出完整三维坐标（X, Y, Z），而非仅平面坐标
  - 标定简单（仅需6个非共面点），适应比赛前快速部署


### 5. 匈牙利匹配追踪算法

基于固定目标池（10个兵种）的多目标追踪策略：
- 融合多维度匹配线索：历史类别置信度、IoU重叠度、3D位置距离、短期轨迹ID
- 状态机管理：INACTIVE→TENTATIVE→CONFIRMED→LOST四状态流转，应对遮挡与丢失
- 猜点逻辑：结合卡尔曼滤波速度预测，在目标丢失时基于运动趋势猜测位置



## 硬件要求

- **GPU**：推荐RTX3060及以上（实测RTX5070/4060笔记本可稳定运行，帧率~10fps）
- **相机**：推荐高分辨率工业相机（如海康CS200，5472×3648分辨率，USB3.0接口）


## 快速开始

### 环境配置

- 目前配置环境仅支持**Ubuntu 22.04**
- 点击这里配置环境: [Environment Setup](./docs/env.md)

### 数字分类模型训练（使用MobileNet-v2）
```python
python -m model.digit_classifier.train
```
其中数据集格式要求为
```
/dataset
  /train
    /B1
    /B2
    /B3
    ...
    /R1
    /R2
    /R3
    ...
  /val
    /B1
    /B2
    /B3
    ...
    /R1
    /R2
    /R3
    ...
```
### 将训练得到的YOLO模型转化为TensorRT
```python
python -m utils.convert_pt2tensorrt
```

### 运行代码
```bash
chmod +x run_scripts/*.sh
./run_scripts/run.sh
```
注意：可在```config/params.yaml```文件中选择使用导入视频（```inference_video```）或者实时相机数据（```streaming_video```）

## 效果展示
![青工会](docs/QGH.jpg)
*青工会展示上榜，效果拔群*

## 联系方式
非常感谢各位对香港科技大学 ENTERPRIZE 战队开源项目的关注与支持，欢迎通过以下方式联系我们, 强烈推荐加入官方开源交流QQ群，获取更多有关机械，硬件，嵌入式与算法的开源信息：
- 作者邮箱: zguobd@connect.ust.hk / szhoubx@connect.ust.hk
- 作者微信: guozilin200429 / s9647814
- 香港科技大学ENTERPRIZE战队开源交流QQ群: 581184202  
- 战队微信公众号: HKUST ENTERPRIZE (ID: gh_855d709c046e)
- 战队Bilibili官号: 港科大ENTERPRIZE战队 (UID: 634988052)

若您在机器人设计过程中受益于我们的项目，恳请在开源引用中注明我们的项目来源,  同时希望能够star⭐⭐我们Github项目❤️❤️。你们的支持是我们不断前行的动力。让我们携手共建一个鼓励创新、协作与超越的 RM 开源生态。🎉🎉🎉

<p align="center">
  <img src="./docs/QQ.jpg" width="300"/>
</p>